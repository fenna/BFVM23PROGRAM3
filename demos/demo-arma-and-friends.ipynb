{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e2ff6ee-d910-47a0-8ccc-5c1dcb7f376f",
   "metadata": {},
   "source": [
    "# ARMA and friends\n",
    "\n",
    "The models we describe in this notebook can be called *classical models*, as they have been worked out at the beginnig of the previous century and therefore have a longer tradition than more recent statistical models.\n",
    "\n",
    "Many time-series exhibit trends and seasonality, while many others assume stationarity. If a time-series is stationary, tis mean and standard deviation stays constant over time. This implies that the time-series has no trend and no cyclic variability. Therefor the removal of irregular components, trends, and seasonal fluctuations is an intrinsic aspect of applying these models. The models then forecast what's left after this removal: business cycles.\n",
    "\n",
    "To apply these classical models, a time-series usually should be decomposed into different components. This is usually done as follows:\n",
    "\n",
    "1. Test for stationarity\n",
    "2. Differencing (if stationarity is detected)\n",
    "3. Fit model and forecast\n",
    "4. Add back the trends and seasonality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a27f82f-df98-4ee5-bfaa-414feca6d193",
   "metadata": {},
   "source": [
    "## Moving Average\n",
    "\n",
    "The *Moving Average* (MA), the unweighted mean over a period of $k$ points, is defined as follows:\n",
    "\n",
    "$$\n",
    "MA = \\frac{1}{k}\\Sigma_{i=1}^{k}x_i\n",
    "$$ \n",
    "\n",
    "where $x_i$ is the observed time-series.\n",
    "\n",
    "The MA can be used to smooth out a time-series, thereby removing noise and periodic fluctuations that occur in the short term, effectively working as a low-pass filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ccbfe6-8c88-437d-bcf8-e7b689bc7693",
   "metadata": {},
   "source": [
    "We can also use MA to forecast into the future as well. The time-series is a linear regression of the current value of the series agaings observed values. For this to work, we can add the [*Autoregressive Model*](https://en.wikipedia.org/wiki/Autoregressive_model), which regresses the variable on its own lagged values: the current value of the value is driven by immediately preceding values using a linear combination. This way we get an autoregressive–moving-average or ARMA-model. \n",
    "\n",
    "In general, the formula for the autocorrelation at lag $k$ is\n",
    "\n",
    "$$\n",
    "r_k = \\frac{\\Sigma_{i=1}^{n-k}(X_i - \\bar{X})(X_{i+k} - \\bar{X})}{\\Sigma_{i=1}^{n}(X_i - \\bar{X})^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ddc58e-37fb-4ee2-97f8-ac2d8e473878",
   "metadata": {},
   "source": [
    "Suppose we have a *very small* timeseries, of only two values: $[3,2]$. Note that in this case $\\bar{X} = 5/2 = 2.5$, and that the variance (the denominator of the equation above) is $(3-2.5)^2 + (5-2.5)^2 = 0.5^2+(−0.5)^2=0.25+0.25=0.5$. \n",
    "\n",
    "The autocorrelation for lag 0 is always one, as a series correlates perfectly with itself:\n",
    "\n",
    "$$\n",
    "r_0 = \\frac{\\Sigma(X_i - \\bar{X})^2}{\\Sigma(X_i - \\bar{X})^2} = 1\n",
    "$$\n",
    "\n",
    "\n",
    "For lag 1, we take the first value again ($X_1=3$) and compare that with the second value ($X_2 = 2$):\n",
    "\n",
    "$$\n",
    "r_1 = \\frac{(3 - 2.5)(2 - 2.5)}{0.5} \n",
    "= \\frac{(0.5)(-0.5)}{0.5} = \\frac{0.25}{0.5} = -0.5\n",
    "$$\n",
    "\n",
    "There are no more lags, so this will be our whole autocorrelation. Let's see this in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99960418-a95d-44ee-aa4c-5ff6298df1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "from statsmodels.graphics.api import qqplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34a0653-4b5b-4c02-a30e-d19ca249a288",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = [3,2]\n",
    "\n",
    "statsmodels.tsa.stattools.acf(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05436976-b556-435d-8bb2-d018434bdaf7",
   "metadata": {},
   "source": [
    "Now let's see how this works in a more realistic scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4ff39f-4867-42bb-8cec-620013fdc4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dta = sm.datasets.sunspots.load_pandas().data\n",
    "#dta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3e7b76-479a-4359-aa4c-790e94e301a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dta.index = pd.Index(sm.tsa.datetools.dates_from_range(\"1700\", \"2008\"))\n",
    "dta.index.freq = dta.index.inferred_freq\n",
    "del dta[\"YEAR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81b3a56-1d20-4a9e-a90d-517ed361cf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "dta.plot(figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d698f5-ee1f-4dfc-a392-f529d56c0fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot an autocorrelation function (ACF) \n",
    "# and a partion autocorrelation function (PACF) plot\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "fig = sm.graphics.tsa.plot_acf(dta.values.squeeze(), lags=40, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = sm.graphics.tsa.plot_pacf(dta, lags=40, ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1702b369-f9fb-4c5c-8ada-e6f2fbf06481",
   "metadata": {},
   "source": [
    "## Interpretation of an ACF-plot\n",
    "\n",
    "- If there is strong positive autocorrelation at a given lag, it means that values at that point are strongly related to previous values.\n",
    "- A strong negative autocorrelation indicates that when one value is high, the next tends to be low.\n",
    "- A rapidly decreasing ACF indicates that the series has little memory, while a slowly decreasing ACF indicates long-term dependencies.\n",
    "\n",
    "## Interpretation of a PACF-plot\n",
    "- A significant PACF at lag $k$ means that there is a direct relationship between $X_t$ and $X_{t−k}$, after correcting for previous lags.\n",
    "- If the PACF quickly goes to zero, this means that the time series can follow an autoregressive (AR) model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
