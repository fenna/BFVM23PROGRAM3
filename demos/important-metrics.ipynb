{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1772eb7-7a2f-42de-91f8-a6a797e15a0f",
   "metadata": {},
   "source": [
    "# Some important statistical metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f72c58-94f0-4b4e-b14e-2e477706eefb",
   "metadata": {},
   "source": [
    "## Mean and Standard Deviation\n",
    "The most important decriptors of continuous data are the *mean* and the *standard deviation*. The mean usually refers to  the arithmetic mean, which is the most commenly used average.\n",
    "\n",
    "$$\n",
    "\\bar{x} = \\frac{1}{n}\\Sigma_{i=1}^{n}x_i\n",
    "$$\n",
    "\n",
    "\n",
    "The standard deviation is the square root of the average squared difference to this mean value:\n",
    "\n",
    "$$\n",
    "\\sigma = \\sqrt{\\frac{1}{n}\\Sigma_{i=1}^{n}(x_i - \\bar{x})^2}\n",
    "= \\sqrt{(E(X-\\bar{x})^2}\n",
    "$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b6577b-37d9-4f75-92a1-d25d0b5d54fb",
   "metadata": {},
   "source": [
    "## Standard Error and Confidence Intervals\n",
    "The *standard error* (SE) is an approximation of the standard deviation of sampled data. It measures the dispersion of sample means around the population mean, but normalized by the root of the sample size. The more data points involved in the calculation, the smaller the standard error tends to be.\n",
    "\n",
    "$$\n",
    "\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}\n",
    "$$\n",
    "\n",
    "An important application of the SE is the estimation of confidence intervals of the mean. A *confidence interval* gives a range of values for a parameter. The 95th percentile upper confidence limit $CI^+$, e.g., is defined as:\n",
    "\n",
    "$$\n",
    "CI^+ = \\bar{x} + (SE \\times 1.96)\n",
    "$$\n",
    "\n",
    "Similary, replacing the plus with a minus, the lower confidence interval is defined as:\n",
    "\n",
    "$$\n",
    "CI^+ = \\bar{x} - (SE \\times 1.96)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406ffa34-a1ea-41a5-91ac-fdcd89cbfbf3",
   "metadata": {},
   "source": [
    "## Median\n",
    "The *median* is another average, particularly useful when the data can't be described accurately by the mean and standard deviations. This is the case when there's a long tail, several peaks, or a skew in one or the other direction. The median is defined as\n",
    "\n",
    "$$\n",
    "mid(X) = x_{\\frac{n+1}{2}}\n",
    "$$\n",
    "\n",
    "This assumes that $X$ is ordered by value in either ascending of descending order. Then, the value that lies in the middle, just at $(n+1)/2$ is the median. \n",
    "\n",
    "The median if the 50th *percentile*, which means that it is higher that exactly half of the point in X. Other important percentiles are the 25th and the 75th, which are also the first and the third *quartiles*. The difference between these two is called the *interquartile range*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcd016c-1fc4-48ae-80c8-033ca2fb2b23",
   "metadata": {},
   "source": [
    "## Colinearity and Correlation Coefficient\n",
    "Collinearity means that independent variables (features) are correlated. This can be problematic in linear models, so if we carry out linear regressio and find two variables that are highly correlated, we could (should) remove of them (or use PCA as a dimensionality reduction technique).\n",
    "\n",
    "The *Pearson correlation coefficient* between two variables $X$ and $Y$ is defined as follows:\n",
    "\n",
    "$$\n",
    "\\rho_{X,Y} = \\frac{cov(X,Y)}{\\sigma_X\\sigma_Y}\n",
    "$$\n",
    "\n",
    "\n",
    "Here, $\\sigma_X$ is the standard deviation of the variable X, and $cov(X,Y)$ is the covariance between the two variables defined as the expected value (the mean) between the differences of each point to the variable mean:\n",
    "\n",
    "$$\n",
    "cov(X,Y) = \\mathbb{E}[(X-\\bar{x})(Y-\\bar{y}]\n",
    "$$\n",
    "\n",
    "Expanded, the formula looks like this\n",
    "\n",
    "$$\n",
    "\\rho_{X,Y} = \\frac{\\Sigma_{i=1}^{n}(x_i - \\bar{x}) (y_i - \\bar{y})}{\\sigma_x\\sigma_y}\n",
    "$$\n",
    "\n",
    "Apart from this metric, we also have the *significance* of the correlation, the so-called *p-value*. This p-value is an indication of how probable it is to find a correlation of $r$ when in reality, there is no connection between the variables (the null-hypothesis: $H_0 : \\rho = 0$).\n",
    "\n",
    "- a low p-value (e.g. < 0.05): the found correlation is probably not by coincidence, we reject $H_0$ and there is a statistical significant connection.\n",
    "- a high p-value (e.g. > 0.05): it is probable that the found correlation is based on coincidence and sufficient evidence for a real connection is lacking.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c80081-fc98-40d9-ae2b-a067dda62ece",
   "metadata": {},
   "source": [
    "## Autocorrelation\n",
    "\n",
    "We can use Pearson correlation to calculate the autocorrelation of the signal: the Pearson correlation between values of the process at different times, as a function of the two times or of the time lag. Let ${ X_t }$ be a random process, and $t$ be any point in time. Then $X_t$ is the value (or realization) produced by a given run of the process at time $t$. Suppose that the process has mean $\\mu_t$ and variance $\\sigma_t^2$ at time $t$, for each $t$. Then the definition of the autocorrelation function between times $t_1$ and $t_2$ is given as \n",
    "\n",
    "$$\n",
    "R_{XX}(t_1, t_2) = \\mathbb{E}[X_{t_1}\\bar{X}_{t_2}]\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
