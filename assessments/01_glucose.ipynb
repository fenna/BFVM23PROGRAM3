{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41cf80da",
   "metadata": {},
   "source": [
    "# Time series: glucose study casus\n",
    "\n",
    "author: F.Feenstra \n",
    "\n",
    "### Domain relevance\n",
    "\n",
    "In life sciences, time series analysis is a method used to examine sequential observations of a specific phenomenon or variable recorded at successive time intervals. Essentially, it involves collecting data points over time, forming a chronological sequence that holds valuable information about dynamic biological processes.\n",
    "\n",
    "For instance, consider diabetes care, where the continuous monitoring of glucose levels at regular intervals throughout the day generates a \"glucose time series.\" Similarly, in medical tests such as electrocardiograms (ECG) and electroencephalograms (EEG), the ongoing recording of heart rhythms or brain waves over specific durations constitutes essential time series data.\n",
    "\n",
    "In its essence, the analysis of time series involves examining these ordered data points to reveal patterns, trends, and changes that happen over a period. It is crucial to acknowledge the biological dynamics contributing to these temporal fluctuations. By measuring signals, such as glucose measurements over time, scientists and doctors gain insight about the complex workings of biological processes. This, in turn, facilitates more precise diagnoses, enables adjustments in treatment strategies, and deepens our understanding of diverse physiological phenomena. The ability to comprehend and interpret time-dependent patterns empowers researchers and practitioners in life sciences to make informed decisions, enhance medical care and contributing to the broader knowledge base of biological systems.\n",
    "\n",
    "The coming two weeks we use glucose data of a subject to learn about time series analysis. We will gain more insights about the glucose level stability of the subject and potential risks. \n",
    "\n",
    "\n",
    "### About the data\n",
    "The datafile we will work with is called `glucose.csv`. [1] It contains data of a subject which is measured continues by means of a sensor, a FreeStyle LibreLink device. The file contains several record types. We are interested in the recordtype that measures the continues glucose values in mmol/L. Furthermore the recordtype that is used for calibration can be used to disgard records of low or uncertain quality.\n",
    "\n",
    "Metadata description:\n",
    "\n",
    "- ID of the row.\n",
    "- Date and time that indicates when the record was taken.\n",
    "- Type of register. The type of registers can take the following values:\n",
    "    0: automatic glucose value register, saved each 15 minutes by the device.\n",
    "    1: manual blood glucose value register, saved in the record after a read by the patient (for calibration purpose)\n",
    "    2: register of insulin without a numeric value.\n",
    "    3: register of carbohydrates without a numeric value.\n",
    "    4: register of insulin done with a numeric value.\n",
    "    5: register of carbohydrates with a numeric value.\n",
    "- Blood glucose value in rows with register type 0 (mg/dl).\n",
    "- Blood glucose value in rows with register type 1 (mg/dl).\n",
    "- Rapid insulin register without a numeric value in rows with register type 2.\n",
    "- Carbohydrates without a numeric value in rows with register type 3.\n",
    "- Units of rapid insulin entered by the patient in rows with register type 4.\n",
    "- Units of carbohydrates entered by the patient in rows with register type 5.\n",
    "\n",
    "The data can be found at `assemblix2019:/data/datasets/Programming/glucose.csv`\n",
    "\n",
    "### Learning outcomes\n",
    "- Apply Python proficiently to clean and structure raw signal data, ensuring it is in a format conducive to analysis\n",
    "- Develop a maintainable and effective preprocessing and evaluation pipeline for time series data\n",
    "- Critically assess trade-offs inherent in different preprocessing strategies, providing insightful justifications for the selected approaches\n",
    "- Implement mathematical algorithms in Python to discern and interpret time series patterns\n",
    "- Design and develop visually appealing and functional data visualizations for time series data\n",
    "- Deliver a well-organized solution that not only solves the given problem but also showcases a systematic and structured approach throughout the entire data processing and analysis pipeline\n",
    "\n",
    "\n",
    "### Instructions\n",
    "1. Copy the original data from assemblix to your data directory\n",
    "2. In the first week read part 1 and conduct the data inspections according the part 1 instructions. Enhance and expand your inspection when needed, but make sure you keep a balance between essential analysis and nice to haves. Be prepared to discuss and demonstrate the solution next week's first session.\n",
    "3. In the second week read part 2 and conduct the data exploration according the part 2 instructions. Enhance and expand your inspection when needed, but make sure you keep a balance between essential analysis and nice to haves. Be prepared to discuss and demonstrate the solution in the third week's first session.\n",
    "4. Upload the solution of this study case in a repository and submit the link to the blackboard assignment. Make sure that your repository is private and invite your teachers and tutors. Please submit your work (even unfinished) before the deadline to receive feedback.\n",
    "5. You are welcome to collaborate in small groups, but please ensure that you acknowledge each member's contributions and engage in discussions to collectively assess the outcomes.\n",
    "   \n",
    "[1] Elouafiq, I. Data & Experiments \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58c7016-55fc-4305-8148-d911380f1c82",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2964c5-f03c-4634-bfcb-e9d49e499a31",
   "metadata": {},
   "source": [
    "## Part 1 (week 1): Data Inspection and Quality Enhancement\n",
    "\n",
    "In Part 1 your objective is to thoroughly inspect and explore the dataset, addressing key questions and ensuring data quality. First you will inspect your dataset, plot your dataset and then you will make decisions about quality enhancement techniques. A good approach is to create inspections and handling functions, it will be easier to re-run certain inspection analysis and explore effect of chosen strategies. Furthermore you might consider visualisations with hoover and span functions to identify records of interest.\n",
    "\n",
    "### Data inspections\n",
    "You are required to inspect at least the following:\n",
    "- Identify the count of observations and the features recorded in the dataset.\n",
    "- Validate if the data types match expectations for each feature.\n",
    "- Assess which columns contain valuable information and explain their significance.\n",
    "- Identify columns with negligible value and articulate reasons for their exclusion from further analysis.\n",
    "- Investigate the presence of duplicate records and determine potential reasons for duplication.\n",
    "- Inspect differences between calibration measures and sensor data (difference and frequency of measurements)\n",
    "- Calculate the percentage of missing data for each feature.\n",
    "- Explore possibilities for imputing missing data, if feasible.\n",
    "- Check for any time gaps in the dataset that may affect the temporal continuity.\n",
    "\n",
    "### Visual Exploration:\n",
    "- Plot glucose levels over time to visually identify trends, patterns, and cycles.\n",
    "- Look for abnormalities or unexpected fluctuations and identify those records\n",
    " \n",
    "### Decision for Quality Enhancement:\n",
    "Based on the inspection, take the following measures for enhancing dataset quality:\n",
    "- Decide what to do with records that have large differences in sensor and calibration measurement.\n",
    "- Remove duplicate records, ensuring data integrity and avoiding redundancy.\n",
    "- Employ suitable imputation methods for handling missing data, balancing accuracy and robustness.\n",
    "- Exclude columns with minimal value to streamline the dataset and improve focus on relevant information.\n",
    "- Address any time gaps in the dataset and develop a strategy how to handle the gaps.\n",
    "- Develop a strategy for managing abnormal glucose readings, considering whether to remove, transform, or investigate further.\n",
    "\n",
    "### Documentation\n",
    "- Document all decisions made during the inspection and enhancement process, providing transparency and aiding reproducibility. Deliver your solution in a repository with a readme file. \n",
    "\n",
    "By means of the steps above we aim to optimize the dataset's quality, making it more reliable to conduct meaningful analyses.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3477cbc7-0aef-42d9-9062-0acc237c40db",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979bcbc5-e082-4b93-9c9f-ed7ec1c2659c",
   "metadata": {},
   "source": [
    "## Part 2 (week 2): Exploring Glucose Time-Series Data\n",
    "\n",
    "Peak detection in glucose time-series data is crucial for recognizing spikes in blood glucose levels, offering insights into the body's responses to events like meals or stress. This analysis supports patients and healthcare providers to make informed decisions about blood glucose management. In this second part, we delve deeper into data exploration, leveraging smoothing and decomposition algorithms to uncover valuable patterns. The steps of the time-of-day analysis is mandatory, but you are free to choose additional explorations. A good approach is to create functions, it will be easier to re-run certain analysis and explore effect of chosen strategies. \n",
    "\n",
    "### Smoothing and Decomposition Algorithms\n",
    "Utilizing smoothing algorithms becomes essential for peak detection, particularly when raw data exhibits fluctuations or short-term variations that might obscure significant patterns. Decomposition further enhances this process by isolating long-term trends from short-term fluctuations, aiding in the identification of peaks within the overall trend component.\n",
    "\n",
    "### Analysis\n",
    "- Determine when glucose levels typically reach their highest and lowest points during the day. Identify potential risks; points ≤50 mg/dL to emphasize occurrence of extreme hypoglycemia and points >300 mg/dL to emphasize occurrence of extreme hyperglycemia.\n",
    "- Explore differences in glucose levels across various hours of the day. Investigate the duration between consecutive peaks to understand patterns in glucose level fluctuations\n",
    "- Stratify the analysis based on weekdays and weekends to capture potential variations.\n",
    "- Calculate meaningful statistics such as % time spent within target range of 70–180 mg/dL, below 70 mg/dL, and above 180 mg/dL (for more inspiration see [2])\n",
    "- Calculate low glucose events. This is defined as when the sensor glucose reading is lower than 70 mg/dL for longer than 15 minutes\n",
    "- Determine if there is a trend\n",
    "\n",
    "\n",
    "### Documentation\n",
    "- Document all decisions made during the explorations, providing transparency and aiding reproducibility. Reflect on the outcomes using captions when appropiate. Deliver your solution in a repository with a readme file. \n",
    "\n",
    "### Challenges \n",
    "In this study case there are several advanced topics you could consider to implement:\n",
    "- An interactive Bokeh plot to dynamically examine the impact of different smoothing algorithms.\n",
    "- An parser object (or set of objects) loading and preprocess the glucose data.\n",
    "\n",
    "\n",
    "[2] Clarke W, Kovatchev B. Statistical tools to analyze continuous glucose monitor data. Diabetes Technol Ther. 2009 Jun;11 Suppl 1(Suppl 1):S45-54. doi: 10.1089/dia.2008.0138. PMID: 19469677; PMCID: PMC2903980.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds2023",
   "language": "python",
   "name": "ds2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
